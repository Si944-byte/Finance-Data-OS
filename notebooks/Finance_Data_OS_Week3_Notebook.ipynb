{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Finance Data OS â€” Week 3 Notebook\n\nPulled 5+ years of real history (**AAPL, MSFT, NVDA, TSLA**) with `yfinance` â†’ **Validation** â†’ **Mart** â†’ **Power BI**.\n\n**Run order:** Imports â†’ Step 1 â†’ Step 2 â†’ Step 3 â†’ Step 4 â†’ Step 5 (optional charts)  \n> Uses `yfinance` with `auto_adjust=True` so **close is already adjusted** (no `Adj_Close`).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Setup â€” Imports & Paths\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\n\n# --- User settings ---\nTICKERS = [\"AAPL\", \"MSFT\", \"NVDA\", \"TSLA\"]\nDATE_START = \"2019-01-01\"\nDATE_END   = \"2025-12-31\"\n\n# Lake & mart locations\nLAKE_ROOT = Path(\"lake/ohlcv\")\nMART_PATH = Path(\"lake/feature_mart.parquet\")\n\nLAKE_ROOT.mkdir(parents=True, exist_ok=True)\nMART_PATH.parent.mkdir(parents=True, exist_ok=True)\n\nprint(\"LAKE_ROOT:\", LAKE_ROOT.resolve())\nprint(\"MART_PATH:\", MART_PATH.resolve())\n\nREQUIRED = [\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"volume\"]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Helpers â€” normalize & types\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Lowercase, flatten any MultiIndex, and map common aliases.\"\"\"\n    if isinstance(df.columns, pd.MultiIndex):\n        df.columns = [\"/\".join([str(x) for x in c if str(x) != \"None\"]) for c in df.columns]\n    df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]\n    # alias\n    alias = {\"symbol\": \"ticker\", \"adj_close\": \"close\"}\n    df = df.rename(columns=alias)\n    return df\n\ndef coerce_types(df: pd.DataFrame) -> pd.DataFrame:\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 1 â€” Pull & Save OHLCV (adjusted)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def pull_ohlcv_adjusted(ticker: str, start: str, end: str) -> pd.DataFrame:\n    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n    df = df.reset_index().rename(columns={\n        \"Date\":\"date\",\"Open\":\"open\",\"High\":\"high\",\"Low\":\"low\",\"Close\":\"close\",\"Volume\":\"volume\"\n    })\n    df = normalize_columns(df)\n    df[\"ticker\"] = ticker\n    df = coerce_types(df)\n    df = df[REQUIRED]\n    return df\n\ndef write_partitioned(df: pd.DataFrame, ticker: str) -> None:\n    for y in sorted(df[\"date\"].dt.year.unique()):\n        out_dir = LAKE_ROOT / f\"ticker={ticker}\" / f\"year={y}\"\n        out_dir.mkdir(parents=True, exist_ok=True)\n        (df[df[\"date\"].dt.year == y]\n            .to_parquet(out_dir / f\"{ticker}_{y}.parquet\", index=False))\n    print(f\"âœ” lake write complete for {ticker}\")\n\nfor t in TICKERS:\n    print(f\"â†» pulling {t}\")\n    _df = pull_ohlcv_adjusted(t, DATE_START, DATE_END)\n    write_partitioned(_df, t)\n\nprint(\"âœ… Step 1 complete\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 2 â€” Repair Legacy Files (safe to re-run)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "files = list(LAKE_ROOT.glob(\"ticker=*/year=*/*.parquet\"))\nprint(\"files to inspect:\", len(files))\n\nfor p in files:\n    df = pd.read_parquet(p)\n    df = coerce_types(normalize_columns(df))\n    part_ticker = p.parent.parent.name.split(\"=\")[1]\n    part_year   = int(p.parent.name.split(\"=\")[1])\n\n    if \"ticker\" not in df.columns:\n        df[\"ticker\"] = part_ticker\n    df[\"ticker\"] = df[\"ticker\"].fillna(part_ticker)\n\n    if all(c in df.columns for c in REQUIRED):\n        df = df[REQUIRED]\n\n    df = df[(df[\"ticker\"] == part_ticker) & (df[\"date\"].dt.year == part_year)]\n    df.to_parquet(p, index=False)\n\nprint(\"âœ… repair pass complete (if needed)\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 3 â€” Validate Lake\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "issues = []\nfiles = list(LAKE_ROOT.glob(\"ticker=*/year=*/*.parquet\"))\nif not files:\n    raise AssertionError(\"No parquet files found under lake/ohlcv.\")\n\nfor p in files:\n    part_ticker = p.parent.parent.name.split(\"=\")[1]\n    part_year   = int(p.parent.name.split(\"=\")[1])\n\n    df = coerce_types(normalize_columns(pd.read_parquet(p)))\n\n    miss = [c for c in REQUIRED if c not in df.columns]\n    if miss:\n        issues.append((str(p), f\"missing cols: {miss}\"))\n        continue\n\n    nulls = df[REQUIRED].isna().sum()\n    if (nulls > 0).any():\n        issues.append((str(p), f\"nulls: {nulls[nulls>0].to_dict()}\"))\n\n    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n        if not pd.api.types.is_numeric_dtype(df[c]):\n            issues.append((str(p), f\"non-numeric {c}: {df[c].dtype}\"))\n\n    if df.duplicated(subset=[\"ticker\",\"date\"]).any():\n        issues.append((str(p), \"duplicate (ticker,date) rows\"))\n\n    if (df[\"ticker\"] != part_ticker).any():\n        issues.append((str(p), f\"wrong ticker values (expected {part_ticker})\"))\n    if (df[\"date\"].dt.year != part_year).any():\n        issues.append((str(p), f\"wrong year values (expected {part_year})\"))\n\nif issues:\n    print(\"âŒ Validation issues:\")\n    for where, msg in issues:\n        print(f\" - [{where}] {msg}\")\n    raise AssertionError(f\"Validation failed: {len(issues)} issue(s).\")\nelse:\n    print(f\"âœ… validation passed ({len(files)} files)\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 4 â€” Build Feature Mart\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_feature_mart() -> pd.DataFrame:\n    dfs = []\n    for fp in LAKE_ROOT.glob(\"ticker=*/year=*/*.parquet\"):\n        part_ticker = fp.parent.parent.name.split(\"=\")[1]\n        part_year   = int(fp.parent.name.split(\"=\")[1])\n\n        df = coerce_types(normalize_columns(pd.read_parquet(fp)))\n        df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\n        r1    = df[\"close\"].pct_change().reset_index(drop=True)\n        sma10 = df[\"close\"].rolling(10, min_periods=10).mean().reset_index(drop=True)\n        vol20 = df[\"close\"].rolling(20, min_periods=20).std(ddof=0).reset_index(drop=True)\n\n        if len(r1) != len(df) or len(sma10) != len(df) or len(vol20) != len(df):\n            raise AssertionError(\n                f\"Index mismatch in partition {part_ticker}-{part_year}: \"\n                f\"len(df)={len(df)} r1={len(r1)} sma10={len(sma10)} vol20={len(vol20)}\"\n            )\n\n        df[\"return1\"] = r1\n        df[\"sma10\"]   = sma10\n        df[\"vol20\"]   = vol20\n        df = df.dropna(subset=[\"return1\",\"sma10\",\"vol20\"])\n\n        dfs.append(df[[\"date\",\"ticker\",\"close\",\"return1\",\"sma10\",\"vol20\"]])\n\n    mart = pd.concat(dfs, ignore_index=True).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\n    if mart.isna().any().any():\n        raise AssertionError(\"Feature mart still contains nulls.\")\n    if mart.duplicated(subset=[\"ticker\",\"date\"]).any():\n        raise AssertionError(\"Duplicate (ticker,date) rows detected in mart.\")\n\n    return mart\n\nmart_df = build_feature_mart()\nmart_df.to_parquet(MART_PATH, index=False)\nprint(\"âœ… Feature mart saved:\", MART_PATH, \"rows:\", len(mart_df))\nmart_df.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 5 â€” Sanity Check & (optional) quick charts\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "for t in TICKERS:\n    d = mart_df[mart_df[\"ticker\"]==t]\n    print(f\"\\n{t}: {len(d)} rows {d['date'].min().date()} â†’ {d['date'].max().date()}\")\n    print(d.head(2)[[\"date\",\"close\",\"return1\",\"sma10\",\"vol20\"]])\n    print(d.tail(2)[[\"date\",\"close\",\"return1\",\"sma10\",\"vol20\"]])\n\n# Optional inline chart (uncomment to show)\n# import matplotlib.pyplot as plt\n# base = mart_df.groupby(\"ticker\")[\"close\"].transform(\"first\")\n# mart_df.assign(norm_close=mart_df[\"close\"]/base).pivot(index=\"date\", columns=\"ticker\", values=\"norm_close\").plot(figsize=(10,4))\n# plt.title(\"Normalized Close (start = 1.0)\")\n# plt.ylabel(\"Normalized\")\n# plt.tight_layout()\n# plt.show()\n\nprint(\"\\nðŸŽ‰ All steps complete.\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}